"""
åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹
ONNX + INT8é‡å­åŒ–ã§é«˜é€Ÿãƒ»ä½ãƒ¡ãƒ¢ãƒªåŒ–
"""
from typing import List
import numpy as np
import os
from pathlib import Path

# ONNXãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
ONNX_MODEL_DIR = Path("onnx_model")

class EmbeddingService:
    def __init__(self, model_name: str = "oshizo/sbert-jsnli-l6-h384-aligned"):
        """
        ONNXãƒ¢ãƒ‡ãƒ«ã§åˆæœŸåŒ–
        """
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.dimension = 384  # SBERTã®å›ºå®šæ¬¡å…ƒæ•°

    def _load_model(self):
        """ONNXãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ã«ãªã£ãŸç¬é–“ã«åˆã‚ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
        if self.model is None:
            print(f"ğŸš€ Loading ONNX model from {ONNX_MODEL_DIR}")
            try:
                from optimum.onnxruntime import ORTModelForFeatureExtraction
                from transformers import AutoTokenizer
                
                # ONNXãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
                self.model = ORTModelForFeatureExtraction.from_pretrained(
                    str(ONNX_MODEL_DIR),
                    provider="CPUExecutionProvider"
                )
                self.tokenizer = AutoTokenizer.from_pretrained(str(ONNX_MODEL_DIR))
                print("âœ… ONNX model loaded successfully")
            except Exception as e:
                print(f"âŒ Failed to load ONNX model: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®sentence-transformers
                print("ğŸ”„ Fallback to sentence-transformers...")
                from sentence_transformers import SentenceTransformer
                self.model = SentenceTransformer(self.model_name)
                self.tokenizer = None
        return self.model
    
    def embed_text(self, text: str) -> np.ndarray:
        """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆONNXå¯¾å¿œï¼‰"""
        model = self._load_model()
        
        if self.tokenizer:
            # ONNXãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=512
            )
            
            with torch.no_grad():
                outputs = model(**inputs)
                # CLSãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
                embeddings = outputs.last_hidden_state[:, 0, :]
                return embeddings.cpu().numpy().squeeze()
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: sentence-transformers
            return model.encode(text, convert_to_numpy=True)
    
    def embed_texts(self, texts: List[str]) -> np.ndarray:
        """è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆONNXå¯¾å¿œï¼‰"""
        model = self._load_model()
        
        if self.tokenizer:
            # ONNXãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
            inputs = self.tokenizer(
                texts, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=512
            )
            
            with torch.no_grad():
                outputs = model(**inputs)
                # CLSãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
                embeddings = outputs.last_hidden_state[:, 0, :]
                return embeddings.cpu().numpy()
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: sentence-transformers
            return model.encode(texts, convert_to_numpy=True)


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_embedding_service = None

def get_embedding_service() -> EmbeddingService:
    global _embedding_service
    if _embedding_service is None:
        # ã“ã“ã§ã¯ã‚¯ãƒ©ã‚¹ã‚’ä½œã‚‹ã ã‘ã§ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¯ã¾ã ã—ãªã„
        _embedding_service = EmbeddingService()
    return _embedding_service